group_size: 4  # how many attempts per sample before one GRPO update
best_metric_strategy: best_reward  # options: best_reward (1 beats -1), best_reward_last (prefer latest among best)

generation:
  do_sample: true
  temperature: 1.0
  top_p: 0.95
  max_new_tokens: 512
  num_beams: 1
  # 固定采样
  # You can add other HF generation kwargs if needed.

grpo:
  beta: 0.04 #之前0.04
  clip_param: 0.2
  normalize_rewards: true
  use_best_of_n: false 
  #reference_model_path: null  # optional: path to frozen reference model; null uses sampling logprobs as reference
  reference_model_path: "/mnt/ssd2/models/Meta-Llama-3.1-8B-Instruct"

lora:
  r: 64 #之前是16 32
  alpha: 128
  dropout: 0.05
  target_modules: ["q_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

optim:
  learning_rate: 2.0e-5 #之前是1e-6
  weight_decay: 0.01
  max_grad_norm: 1 #之前是100
  num_train_epochs: 1
  gradient_accumulation_steps: 1

save:
  lora_output_dir: "outputs/{TIMESTAMP}/grpo_lora"
