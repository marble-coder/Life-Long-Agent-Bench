import os
import json
import torch
import logging
from typing import Optional, Dict, Any, List
from datetime import datetime

# ç¡®ä¿å·²å®‰è£…: pip install peft transformers accelerate
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from transformers import TrainingArguments, Trainer, TrainerCallback
from torch.utils.data import Dataset

from src.callbacks.callback import Callback, CallbackArguments
from src.typings import Session, SessionEvaluationOutcome, SampleStatus
from src.language_models.instance.huggingface_language_model import HuggingfaceLanguageModel

logger = logging.getLogger(__name__)


class SftTrajectoryDataset(Dataset):
    """
    ç”¨äº SFT çš„è½¨è¿¹æ•°æ®é›†
    å…¼å®¹æ‰€æœ‰æ”¯æŒ apply_chat_template çš„æ¨¡å‹ï¼ˆLlama, Qwen, Mistral ç­‰ï¼‰
    """
    def __init__(self, trajectories: List[Session], tokenizer, max_length=2048):
        self.trajectories = trajectories
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.trajectories)

    # def __getitem__(self, idx):
    #     session = self.trajectories[idx]
        
    #     # å°† ChatHistory è½¬æ¢ä¸ºæ ‡å‡†çš„ messages æ ¼å¼
    #     messages = []
    #     for item in session.chat_history.value:
    #         messages.append({
    #             "role": item.role.value,  # "user" æˆ– "agent"
    #             "content": item.content
    #         })
        
    #     # ä½¿ç”¨ tokenizer çš„èŠå¤©æ¨¡æ¿ï¼ˆè‡ªåŠ¨é€‚é… Llama/Qwen/Mistral ç­‰ï¼‰
    #     # æ³¨æ„ï¼šä¸åŒæ¨¡å‹çš„ tokenizer ä¼šè‡ªåŠ¨ä½¿ç”¨å„è‡ªçš„ç‰¹æ®Š token
    #     text = self.tokenizer.apply_chat_template(
    #         messages,
    #         tokenize=False,
    #         add_generation_prompt=False
    #     )
        
    #     # Tokenize
    #     encoding = self.tokenizer(
    #         text,
    #         truncation=True,
    #         max_length=self.max_length,
    #         padding="max_length",
    #         return_tensors="pt"
    #     )
        
    #     input_ids = encoding["input_ids"].squeeze(0)
    #     attention_mask = encoding["attention_mask"].squeeze(0)
        
    #     # å¯¹äº Causal LMï¼Œlabels å°±æ˜¯ input_ids
    #     # ä½†æˆ‘ä»¬éœ€è¦å°† padding éƒ¨åˆ†çš„ label è®¾ä¸º -100ï¼ˆå¿½ç•¥ lossï¼‰
    #     labels = input_ids.clone()
    #     labels[attention_mask == 0] = -100
        
    #     return {
    #         "input_ids": input_ids,
    #         "attention_mask": attention_mask,
    #         "labels": labels
    #     }
    # âœ… æ­£ç¡®çš„è®¿é—®æ–¹å¼
    def __getitem__(self, idx):
        session = self.trajectories[idx]
        
        # å°† ChatHistory è½¬æ¢ä¸ºæ ‡å‡†çš„ messages æ ¼å¼
        messages = []
        for item_index in range(session.chat_history.get_value_length()):
            item = session.chat_history.get_item_deep_copy(item_index)
            # ç»Ÿä¸€è§’è‰²åç§°ï¼šagent -> assistant
            role = "assistant" if item.role.value == "agent" else item.role.value
            messages.append({
                "role": role,
                "content": item.content
            })
        
        # ä½¿ç”¨ tokenizer çš„èŠå¤©æ¨¡æ¿ï¼ˆè‡ªåŠ¨é€‚é… Llama/Qwen ç­‰ï¼‰
        try:
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
        except Exception as e:
            # å¦‚æœ apply_chat_template å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ‹¼æ¥
            logger.warning(f"apply_chat_template å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼: {e}")
            text = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
        
        # ... åç»­ä»£ç ä¿æŒä¸å˜
        # Tokenize
        # âœ… å…³é”®ä¿®å¤ï¼šç¡®ä¿ tokenizer æœ‰ pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)
        
        # å¯¹äº Causal LMï¼Œlabels å°±æ˜¯ input_ids
        # ä½†æˆ‘ä»¬éœ€è¦å°† padding éƒ¨åˆ†çš„ label è®¾ä¸º -100ï¼ˆå¿½ç•¥ lossï¼‰
        labels = input_ids.clone()
        labels[attention_mask == 0] = -100
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    


class LossLoggingCallback(TrainerCallback):
    """è®°å½•è®­ç»ƒ loss åˆ° CSV æ–‡ä»¶"""
    def __init__(self, loss_log_path: str, batch_id: int):
        self.loss_log_path = loss_log_path
        self.batch_id = batch_id
        
        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ª batchï¼Œå†™å…¥ header
        if not os.path.exists(self.loss_log_path):
            os.makedirs(os.path.dirname(self.loss_log_path), exist_ok=True)
            with open(self.loss_log_path, "w") as f:
                f.write("batch_id,step,loss,learning_rate,timestamp\n")

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and 'loss' in logs:
            with open(self.loss_log_path, "a") as f:
                step = state.global_step
                loss = logs['loss']
                lr = logs.get('learning_rate', 'N/A')
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"{self.batch_id},{step},{loss},{lr},{timestamp}\n")


class TestTimeTrainingCallback(Callback):
    """
    Test-Time Training with LoRA
    
    ç‰¹æ€§ï¼š
    1. æ¨¡å‹æ— å…³ï¼šå…¼å®¹ Llama, Qwen, Mistral ç­‰æ‰€æœ‰ HuggingFace æ¨¡å‹
    2. æ¸è¿›å¼å­¦ä¹ ï¼šLoRA æƒé‡æŒç»­ç´¯ç§¯æ›´æ–°
    3. å®Œæ•´å¯è¿½æº¯ï¼šæ¯ä¸ª batch çš„æ•°æ®å’Œ loss éƒ½å•ç‹¬ä¿å­˜
    4. çŠ¶æ€å¯æ¢å¤ï¼šæ”¯æŒå®éªŒä¸­æ–­åç»§ç»­
    """
    def __init__(
        self,
        batch_size: int = 8,
        sft_data_dir: str = "outputs/{TIMESTAMP}/sft_data",
        loss_log_path: str = "outputs/{TIMESTAMP}/loss_log.csv",
        lora_r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.1,
        lora_target_modules: Optional[List[str]] = None,
        learning_rate: float = 2e-4,
        num_train_epochs: int = 1,
        per_device_train_batch_size: int = 1,
        gradient_accumulation_steps: int = 4,
        max_seq_length: int = 2048,
    ):
        super().__init__()
        self.batch_size = batch_size
        # ä¿å­˜åŸå§‹è·¯å¾„æ¨¡æ¿ï¼ˆå¸¦å ä½ç¬¦ï¼‰
        self.sft_data_dir_template = sft_data_dir
        self.loss_log_path_template = loss_log_path
        # å®é™…è·¯å¾„å°†åœ¨é¦–æ¬¡è¿è¡Œæ—¶åˆå§‹åŒ–
        self.sft_data_dir = None
        self.loss_log_path = None
        self.trainer_output_dir = None  # æ–°å¢ï¼šTrainerä¸´æ—¶ç›®å½•
        self.max_seq_length = max_seq_length
        
        # LoRA é…ç½®
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        # å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨é€šç”¨çš„ target_modulesï¼ˆé€‚é…å¤§å¤šæ•°æ¨¡å‹ï¼‰
        self.lora_target_modules = lora_target_modules or [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"  # é€‚é… Llama/Qwen çš„ MLP
        ]
        
        # è®­ç»ƒé…ç½®
        self.learning_rate = learning_rate
        self.num_train_epochs = num_train_epochs
        self.per_device_train_batch_size = per_device_train_batch_size
        self.gradient_accumulation_steps = gradient_accumulation_steps
        
        # å†…éƒ¨çŠ¶æ€
        self.successful_trajectories: List[Session] = []
        self.model_ref = None
        self.tokenizer_ref = None
        self.is_lora_applied = False
        self.training_batch_count = 0
        self.paths_initialized = False  # âœ… æ·»åŠ è¿™ä¸€è¡Œï¼

    @classmethod
    def is_unique(cls) -> bool:
        return True

    def on_task_complete(self, callback_args: CallbackArguments) -> None:
        """åœ¨æ¯ä¸ªæ ·æœ¬å®Œæˆåï¼Œæ£€æŸ¥æ˜¯å¦æˆåŠŸå¹¶æ”¶é›†è½¨è¿¹"""
        session = callback_args.current_session
        
        # åªæ”¶é›†æˆåŠŸçš„è½¨è¿¹
        if (
            session.evaluation_record.outcome == SessionEvaluationOutcome.CORRECT and
            session.sample_status == SampleStatus.COMPLETED
        ):
            print(f"âœ… [TTT] æˆåŠŸè½¨è¿¹æ”¶é›†: sample_index={session.sample_index}")
            self.successful_trajectories.append(session.model_copy(deep=True))
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ° batch_size
            if len(self.successful_trajectories) >= self.batch_size:
                print(f"\n{'='*60}")
                print(f"ğŸ¯ [TTT] å·²æ”¶é›† {len(self.successful_trajectories)} æ¡æˆåŠŸè½¨è¿¹")
                print(f"{'='*60}\n")
                self._run_training(callback_args)

    # def _run_training(self, callback_args: CallbackArguments):
    #     """æ‰§è¡Œä¸€æ¬¡ LoRA å¾®è°ƒ"""
    #     print(f"ğŸš€ [TTT] å¼€å§‹ç¬¬ {self.training_batch_count + 1} è½® LoRA è®­ç»ƒ...")
        
    #     # 1. è·å–æ¨¡å‹å’Œ tokenizer å¼•ç”¨
    #     if not self._initialize_model_refs(callback_args):
    #         return
        
    #     # 2. é¦–æ¬¡åº”ç”¨ LoRAï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è®­ç»ƒæ—¶ï¼‰
    #     if not self.is_lora_applied:
    #         self._apply_lora_to_model()
        
    #     # 3. ä¿å­˜å½“å‰ batch çš„è®­ç»ƒæ•°æ®
    #     self._save_sft_batch_data()
        
    #     # 4. å‡†å¤‡æ•°æ®é›†
    #     train_dataset = SftTrajectoryDataset(
    #         self.successful_trajectories,
    #         self.tokenizer_ref,
    #         max_length=self.max_seq_length
    #     )
        
    #     # 5. é…ç½®è®­ç»ƒå‚æ•°
    #     training_args = TrainingArguments(
    #         output_dir=os.path.join(
    #             os.path.dirname(self.loss_log_path),
    #             f"ttt_trainer_batch_{self.training_batch_count}"
    #         ),
    #         num_train_epochs=self.num_train_epochs,
    #         per_device_train_batch_size=self.per_device_train_batch_size,
    #         gradient_accumulation_steps=self.gradient_accumulation_steps,
    #         learning_rate=self.learning_rate,
    #         logging_steps=1,
    #         save_strategy="no",
    #         report_to="none",
    #         remove_unused_columns=False,
    #     )
        
    #     # 6. åˆ›å»º Trainer å¹¶æ³¨å…¥ loss æ—¥å¿—å›è°ƒ
    #     loss_logger = LossLoggingCallback(self.loss_log_path, self.training_batch_count)
    #     trainer = Trainer(
    #         model=self.model_ref,
    #         args=training_args,
    #         train_dataset=train_dataset,
    #         callbacks=[loss_logger]
    #     )
        
    #     # 7. å¼€å§‹è®­ç»ƒ
    #     print(f"ğŸ‹ï¸  [TTT] è®­ç»ƒä¸­...")
    #     train_result = trainer.train()
    #     print(f"âœ… [TTT] è®­ç»ƒå®Œæˆ! Loss: {train_result.training_loss:.4f}")
    #     print(f"ğŸ“Š [TTT] Loss å·²è®°å½•è‡³: {self.loss_log_path}")
        
    #     # 8. æ¸…ç†å½“å‰ batchï¼Œå‡†å¤‡ä¸‹ä¸€è½®
    #     self.successful_trajectories = []
    #     self.training_batch_count += 1
    #     print(f"ğŸ”„ [TTT] æ¸…ç†å®Œæˆï¼Œç»§ç»­æ”¶é›†ä¸‹ä¸€æ‰¹è½¨è¿¹\n")
    def _run_training(self, callback_args: CallbackArguments):
        """æ‰§è¡Œä¸€æ¬¡ LoRA å¾®è°ƒ"""
        # é¦–å…ˆåˆå§‹åŒ–è·¯å¾„
        self._initialize_paths()
        
        print(f"ğŸš€ [TTT] å¼€å§‹ç¬¬ {self.training_batch_count + 1} è½® LoRA è®­ç»ƒ...")
        
        # 1. è·å–æ¨¡å‹å’Œ tokenizer å¼•ç”¨
        if not self._initialize_model_refs(callback_args):
            return
        
        # 2. é¦–æ¬¡åº”ç”¨ LoRAï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è®­ç»ƒæ—¶ï¼‰
        if not self.is_lora_applied:
            self._apply_lora_to_model()
        
        # 3. ä¿å­˜å½“å‰ batch çš„è®­ç»ƒæ•°æ®
        self._save_sft_batch_data()
        
        # 4. å‡†å¤‡æ•°æ®é›†
        train_dataset = SftTrajectoryDataset(
            self.successful_trajectories,
            self.tokenizer_ref,
            max_length=self.max_seq_length
        )
        
        # 5. é…ç½®è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.trainer_output_dir,  # âœ… æ‰€æœ‰batchå…±ç”¨ä¸€ä¸ªç›®å½•
            num_train_epochs=self.num_train_epochs,
            per_device_train_batch_size=self.per_device_train_batch_size,
            gradient_accumulation_steps=self.gradient_accumulation_steps,
            learning_rate=self.learning_rate,
            logging_steps=1,
            save_strategy="no",
            report_to="none",
            remove_unused_columns=False,
            overwrite_output_dir=True,  # âœ… å…è®¸è¦†ç›–ï¼ˆå› ä¸ºå…±ç”¨ç›®å½•ï¼‰
        )
        
        # 6. åˆ›å»º Trainer å¹¶æ³¨å…¥ loss æ—¥å¿—å›è°ƒ
        loss_logger = LossLoggingCallback(self.loss_log_path, self.training_batch_count)
        trainer = Trainer(
            model=self.model_ref,
            args=training_args,
            train_dataset=train_dataset,
            callbacks=[loss_logger]
        )
        
        # 7. å¼€å§‹è®­ç»ƒ
        print(f"ğŸ‹ï¸  [TTT] è®­ç»ƒä¸­...")
        train_result = trainer.train()
        print(f"âœ… [TTT] è®­ç»ƒå®Œæˆ! Loss: {train_result.training_loss:.4f}")
        print(f"ğŸ“Š [TTT] Loss å·²è®°å½•è‡³: {self.loss_log_path}")
        
        # 8. æ¸…ç†å½“å‰ batchï¼Œå‡†å¤‡ä¸‹ä¸€è½®
        self.successful_trajectories = []
        self.training_batch_count += 1
        print(f"ğŸ”„ [TTT] æ¸…ç†å®Œæˆï¼Œç»§ç»­æ”¶é›†ä¸‹ä¸€æ‰¹è½¨è¿¹\n")
        
    def _initialize_paths(self):
        """åˆå§‹åŒ–å®é™…çš„è¾“å‡ºè·¯å¾„ï¼ˆä»state_diræå–ï¼‰"""
        if self.paths_initialized:
            return
        
        # ä» state_dir æå–å®é™…çš„ output_dir
        # state_dir æ ¼å¼: outputs/2025-10-14-13-24-48/callback_state/callback_3
        state_dir = self.get_state_dir()
        output_dir = os.path.dirname(os.path.dirname(state_dir))  # å‘ä¸Šä¸¤çº§
        
        # æ›¿æ¢å ä½ç¬¦ï¼Œä½¿ç”¨å®é™…çš„æ—¶é—´æˆ³ç›®å½•
        self.sft_data_dir = os.path.join(output_dir, "sft_data")
        self.loss_log_path = os.path.join(output_dir, "loss_log.csv")
        # æ‰€æœ‰batchå…±ç”¨ä¸€ä¸ªä¸´æ—¶ç›®å½•ï¼ˆé¿å…äº§ç”Ÿå¤§é‡ç©ºç›®å½•ï¼‰
        self.trainer_output_dir = os.path.join(output_dir, ".ttt_trainer_temp")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(self.sft_data_dir, exist_ok=True)
        os.makedirs(os.path.dirname(self.loss_log_path), exist_ok=True)
        os.makedirs(self.trainer_output_dir, exist_ok=True)
        
        print(f"ğŸ“ [TTT] è¾“å‡ºç›®å½•å·²åˆå§‹åŒ–:")
        print(f"   - SFTæ•°æ®: {self.sft_data_dir}")
        print(f"   - Lossæ—¥å¿—: {self.loss_log_path}")
        print(f"   - Trainerä¸´æ—¶ç›®å½•: {self.trainer_output_dir}")
        
        self.paths_initialized = True

    def _initialize_model_refs(self, callback_args: CallbackArguments) -> bool:
        """åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer çš„å¼•ç”¨"""
        if self.model_ref is not None:
            return True
        
        agent = callback_args.session_context.agent
        if not hasattr(agent, "_language_model"):
            print("âš ï¸  [TTT] Agent æ²¡æœ‰ _language_model å±æ€§ï¼Œè·³è¿‡è®­ç»ƒ")
            return False
        
        if not isinstance(agent._language_model, HuggingfaceLanguageModel):
            print("âš ï¸  [TTT] åªæ”¯æŒ HuggingfaceLanguageModelï¼Œè·³è¿‡è®­ç»ƒ")
            return False
        
        self.model_ref = agent._language_model.model
        self.tokenizer_ref = agent._language_model.tokenizer
        
        # æ£€æµ‹æ¨¡å‹ç±»å‹ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        model_type = self.model_ref.config.model_type
        print(f"ğŸ” [TTT] æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type}")
        
        return True

    def _apply_lora_to_model(self):
        """é¦–æ¬¡åº”ç”¨ LoRA é€‚é…å™¨åˆ°æ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"ğŸ”§ [TTT] é¦–æ¬¡åº”ç”¨ LoRA é€‚é…å™¨")
        print(f"{'='*60}")
        
        # åˆ›å»º LoRA é…ç½®
        lora_config = LoraConfig(
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            target_modules=self.lora_target_modules,
            lora_dropout=self.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # åº”ç”¨ LoRA
        self.model_ref = get_peft_model(self.model_ref, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
        self.model_ref.print_trainable_parameters()
        
        # å…³é”®ï¼šæ›´æ–° agent ä¸­çš„æ¨¡å‹å¼•ç”¨
        # è¿™æ ·åç»­çš„æ¨ç†ä¼šä½¿ç”¨å¸¦ LoRA çš„æ¨¡å‹
        agent = self.model_ref  # å·²ç»æ˜¯ PeftModel äº†
        
        self.is_lora_applied = True
        print(f"âœ… [TTT] LoRA é€‚é…å™¨å·²æˆåŠŸåº”ç”¨\n")

    def _save_sft_batch_data(self):
        """ä¿å­˜å½“å‰ batch çš„ SFT æ•°æ®"""
        batch_file = os.path.join(
            self.sft_data_dir,
            f"batch_{self.training_batch_count:03d}.json"
        )
        os.makedirs(os.path.dirname(batch_file), exist_ok=True)
        
        # ä¿å­˜å®Œæ•´çš„ Session æ•°æ®
        batch_data = {
            "batch_id": self.training_batch_count,
            "sample_count": len(self.successful_trajectories),
            "sample_indices": [s.sample_index for s in self.successful_trajectories],
            "trajectories": [s.model_dump() for s in self.successful_trajectories]
        }
        
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(batch_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ [TTT] Batch {self.training_batch_count} æ•°æ®å·²ä¿å­˜: {batch_file}")

    def on_state_save(self, callback_args: CallbackArguments) -> None:
        """ä¿å­˜å›è°ƒçŠ¶æ€ï¼ˆæ”¯æŒæ–­ç‚¹æ¢å¤ï¼‰"""
        state = {
            "training_batch_count": self.training_batch_count,
            "is_lora_applied": self.is_lora_applied,
            "successful_trajectories": [s.model_dump() for s in self.successful_trajectories]
        }
        state_path = os.path.join(self.get_state_dir(), "ttt_state.json")
        with open(state_path, 'w') as f:
            json.dump(state, f, indent=2)

    # def restore_state(self) -> None:
    #     """æ¢å¤å›è°ƒçŠ¶æ€"""
    #     state_path = os.path.join(self.get_state_dir(), "ttt_state.json")
    #     if os.path.exists(state_path):
    #         with open(state_path, 'r') as f:
    #             state = json.load(f)
    #         self.training_batch_count = state.get("training_batch_count", 0)
    #         self.is_lora_applied = state.get("is_lora_applied", False)
    #         self.successful_trajectories = [
    #             Session.model_validate(s)
    #             for s in state.get("successful_trajectories", [])
    #         ]
    #         print(f"ğŸ”„ [TTT] çŠ¶æ€å·²æ¢å¤: å·²å®Œæˆ {self.training_batch_count} è½®è®­ç»ƒï¼Œ"
    #               f"å½“å‰æ”¶é›† {len(self.successful_trajectories)} æ¡è½¨è¿¹")
    def restore_state(self) -> None:
        """æ¢å¤å›è°ƒçŠ¶æ€"""
        # å…ˆåˆå§‹åŒ–è·¯å¾„
        self._initialize_paths()
        
        state_path = os.path.join(self.get_state_dir(), "ttt_state.json")
        if os.path.exists(state_path):
            with open(state_path, 'r') as f:
                state = json.load(f)
            self.training_batch_count = state.get("training_batch_count", 0)
            self.is_lora_applied = state.get("is_lora_applied", False)
            self.successful_trajectories = [
                Session.model_validate(s)
                for s in state.get("successful_trajectories", [])
            ]
            print(f"ğŸ”„ [TTT] çŠ¶æ€å·²æ¢å¤: å·²å®Œæˆ {self.training_batch_count} è½®è®­ç»ƒï¼Œ"
                f"å½“å‰æ”¶é›† {len(self.successful_trajectories)} æ¡è½¨è¿¹")