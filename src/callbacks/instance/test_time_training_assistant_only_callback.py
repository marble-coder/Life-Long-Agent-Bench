import os
import json
import torch
import logging
from typing import Optional, Dict, Any, List
from datetime import datetime

# ç¡®ä¿å·²å®‰è£…: pip install peft transformers accelerate
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from transformers import TrainingArguments, Trainer, TrainerCallback
from torch.utils.data import Dataset

from src.callbacks.callback import Callback, CallbackArguments
from src.typings import Session, SessionEvaluationOutcome, SampleStatus
from src.language_models.instance.huggingface_language_model import HuggingfaceLanguageModel

logger = logging.getLogger(__name__)


class SftTrajectoryDataset(Dataset):
    """
    ç”¨äº SFT çš„è½¨è¿¹æ•°æ®é›†ï¼ˆåªè®­ç»ƒ Assistant è¾“å‡ºï¼‰
    å…¼å®¹æ‰€æœ‰æ”¯æŒ apply_chat_template çš„æ¨¡å‹ï¼ˆLlama, Qwen, Mistral ç­‰ï¼‰
    """
    def __init__(self, trajectories: List[Session], tokenizer, max_length=2048):
        self.trajectories = trajectories
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.trajectories)

    def __getitem__(self, idx):
        session = self.trajectories[idx]
        
        # å°† ChatHistory è½¬æ¢ä¸ºæ ‡å‡†çš„ messages æ ¼å¼
        messages = []
        for item_index in range(session.chat_history.get_value_length()):
            item = session.chat_history.get_item_deep_copy(item_index)
            # ç»Ÿä¸€è§’è‰²åç§°ï¼šagent -> assistant
            role = "assistant" if item.role.value == "agent" else item.role.value
            messages.append({
                "role": role,
                "content": item.content
            })
        
        # ä½¿ç”¨ tokenizer çš„èŠå¤©æ¨¡æ¿ï¼ˆè‡ªåŠ¨é€‚é… Llama/Qwen ç­‰ï¼‰
        try:
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
        except Exception as e:
            # å¦‚æœ apply_chat_template å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ‹¼æ¥
            logger.warning(f"apply_chat_template å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼: {e}")
            text = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
        
        # Tokenize
        # âœ… å…³é”®ä¿®å¤ï¼šç¡®ä¿ tokenizer æœ‰ pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)
        
        # âœ… å…³é”®ï¼šåªè®­ç»ƒ assistant çš„è¾“å‡ºï¼Œmask æ‰ user input
        labels = input_ids.clone()
        
        # è¯†åˆ«å“ªäº› token å±äº assistant æ¶ˆæ¯
        assistant_mask = self._identify_assistant_tokens(messages, input_ids)
        
        # è®¾ç½® labelsï¼šåªåœ¨ assistant éƒ¨åˆ†è®¡ç®— loss
        # -100 è¡¨ç¤ºå¿½ç•¥è¯¥ä½ç½®çš„ loss
        labels[~assistant_mask] = -100  # é assistant éƒ¨åˆ†è®¾ä¸º -100
        labels[attention_mask == 0] = -100    # padding éƒ¨åˆ†è®¾ä¸º -100
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    
    def _identify_assistant_tokens(self, messages, full_input_ids):
        """
        è¯†åˆ«å®Œæ•´åºåˆ—ä¸­å“ªäº› token å±äº assistant æ¶ˆæ¯
        è¿”å›ä¸€ä¸ªä¸ input_ids é•¿åº¦ç›¸åŒçš„å¸ƒå°” mask
        """
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åˆå§‹åŒ– maskï¼šå…¨ä¸º Falseï¼ˆè¡¨ç¤ºéƒ½ä¸æ˜¯ assistantï¼‰
        assistant_mask = torch.zeros(len(full_input_ids), dtype=torch.bool)
        
        # è·å–å®é™…åºåˆ—é•¿åº¦ï¼ˆå»é™¤ paddingï¼‰
        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id
        actual_length = (full_input_ids != pad_token_id).sum().item()
        if actual_length == 0:
            return assistant_mask
        
        # æ–¹æ³•ï¼šé€šè¿‡é€æ­¥æ„å»ºå¯¹è¯æ¥è¯†åˆ«æ¯ä¸ªæ¶ˆæ¯çš„tokenèŒƒå›´
        try:
            # é¦–å…ˆè·å–å®Œæ•´åºåˆ—çš„token idsï¼ˆç”¨äºåŒ¹é…ï¼‰
            full_tokens = full_input_ids[:actual_length].tolist()
            
            # é€æ­¥æ„å»ºï¼Œæ‰¾åˆ°æ¯ä¸ªæ¶ˆæ¯çš„è¾¹ç•Œ
            prev_tokens = []
            
            for msg_idx in range(len(messages)):
                # æ„å»ºåˆ°å½“å‰æ¶ˆæ¯ä¸ºæ­¢çš„å¯¹è¯
                partial_messages = messages[:msg_idx + 1]
                
                try:
                    partial_text = self.tokenizer.apply_chat_template(
                        partial_messages,
                        tokenize=False,
                        add_generation_prompt=False
                    )
                except:
                    partial_text = "\n".join([f"{m['role']}: {m['content']}" for m in partial_messages])
                
                # Tokenize éƒ¨åˆ†å¯¹è¯ï¼ˆä¸æ·»åŠ special tokensï¼Œå› ä¸ºå®Œæ•´åºåˆ—å·²ç»åŒ…å«äº†ï¼‰
                partial_encoding = self.tokenizer(
                    partial_text,
                    truncation=True,
                    max_length=self.max_length,
                    add_special_tokens=False,
                    return_tensors="pt"
                )
                partial_tokens = partial_encoding["input_ids"].squeeze(0).tolist()
                
                # åœ¨å½“å‰æ¶ˆæ¯ä¹‹å‰çš„æ‰€æœ‰token
                if msg_idx == 0:
                    msg_start = 0
                else:
                    # åœ¨å®Œæ•´åºåˆ—ä¸­æ‰¾åˆ°prev_tokensçš„ä½ç½®
                    msg_start = self._find_token_sequence(full_tokens, prev_tokens, 0)
                    if msg_start == -1:
                        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨å‰ä¸€ä¸ªæ¶ˆæ¯çš„ç»“æŸä½ç½®ä¼°è®¡
                        msg_start = len(prev_tokens)
                
                # åœ¨å®Œæ•´åºåˆ—ä¸­æ‰¾åˆ°partial_tokensçš„ä½ç½®ï¼ˆä»msg_startå¼€å§‹ï¼‰
                actual_start = self._find_token_sequence(full_tokens, partial_tokens, max(0, msg_start - 10))
                if actual_start == -1:
                    # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨å¢é‡æ–¹å¼
                    actual_start = len(prev_tokens)
                
                msg_end = min(actual_start + len(partial_tokens), actual_length)
                msg_start = actual_start
                
                # å¦‚æœå½“å‰æ¶ˆæ¯æ˜¯ assistantï¼Œæ ‡è®°å¯¹åº”çš„ token
                if messages[msg_idx]["role"] == "assistant":
                    if msg_start < actual_length:
                        assistant_mask[msg_start:msg_end] = True
                
                # ä¿å­˜å½“å‰tokenåºåˆ—ç”¨äºä¸‹ä¸€æ¬¡è¿­ä»£
                prev_tokens = partial_tokens[:]
                
                # å¦‚æœå·²ç»è¾¾åˆ°åºåˆ—é•¿åº¦ï¼Œæå‰é€€å‡º
                if len(partial_tokens) >= actual_length:
                    break
                    
        except Exception as e:
            # å¦‚æœè¯†åˆ«å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•ç­–ç•¥
            logger.warning(f"è¯†åˆ« assistant tokens å¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥: {e}")
            # å›é€€ç­–ç•¥ï¼šé€šè¿‡æŸ¥æ‰¾assistantæ¶ˆæ¯çš„æ–‡æœ¬æ¨¡å¼æ¥è¯†åˆ«
            # è¿™å–å†³äºå…·ä½“çš„tokenizeræ ¼å¼ï¼Œä½†è‡³å°‘èƒ½å¤„ç†ä¸€éƒ¨åˆ†æƒ…å†µ
            try:
                # å°è¯•é€šè¿‡ç›´æ¥åŒ¹é…assistantæ¶ˆæ¯å†…å®¹æ¥è¯†åˆ«
                for msg in messages:
                    if msg["role"] == "assistant":
                        # Tokenize assistantæ¶ˆæ¯å†…å®¹
                        content_tokens = self.tokenizer.encode(
                            msg["content"],
                            add_special_tokens=False
                        )
                        # åœ¨å®Œæ•´åºåˆ—ä¸­æŸ¥æ‰¾ï¼ˆç®€å•åŒ¹é…ï¼‰
                        # æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•ä¸å¤Ÿç²¾ç¡®ï¼Œä½†ä½œä¸ºå›é€€æ–¹æ¡ˆ
                        for i in range(len(full_tokens) - len(content_tokens) + 1):
                            if full_tokens[i:i+len(content_tokens)] == content_tokens:
                                assistant_mask[i:i+len(content_tokens)] = True
                                break
            except:
                # æœ€åçš„å›é€€ï¼šå‡è®¾å50%æ˜¯assistant
                mid_point = actual_length // 2
                assistant_mask[mid_point:actual_length] = True
        
        return assistant_mask
    
    def _find_token_sequence(self, full_tokens, sequence, start_pos=0):
        """åœ¨å®Œæ•´åºåˆ—ä¸­æŸ¥æ‰¾å­åºåˆ—çš„ä½ç½®"""
        if not sequence:
            return start_pos
        for i in range(start_pos, len(full_tokens) - len(sequence) + 1):
            if full_tokens[i:i+len(sequence)] == sequence:
                return i
        return -1


class LossLoggingCallback(TrainerCallback):
    """è®°å½•è®­ç»ƒ loss åˆ° CSV æ–‡ä»¶"""
    def __init__(self, loss_log_path: str, batch_id: int):
        self.loss_log_path = loss_log_path
        self.batch_id = batch_id
        
        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ª batchï¼Œå†™å…¥ header
        if not os.path.exists(self.loss_log_path):
            os.makedirs(os.path.dirname(self.loss_log_path), exist_ok=True)
            with open(self.loss_log_path, "w") as f:
                f.write("batch_id,step,loss,learning_rate,timestamp\n")

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and 'loss' in logs:
            with open(self.loss_log_path, "a") as f:
                step = state.global_step
                loss = logs['loss']
                lr = logs.get('learning_rate', 'N/A')
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"{self.batch_id},{step},{loss},{lr},{timestamp}\n")


class TestTimeTrainingAssistantOnlyCallback(Callback):
    """
    Test-Time Training with LoRA (åªè®­ç»ƒ Assistant è¾“å‡º)
    
    ç‰¹æ€§ï¼š
    1. æ¨¡å‹æ— å…³ï¼šå…¼å®¹ Llama, Qwen, Mistral ç­‰æ‰€æœ‰ HuggingFace æ¨¡å‹
    2. æ¸è¿›å¼å­¦ä¹ ï¼šLoRA æƒé‡æŒç»­ç´¯ç§¯æ›´æ–°
    3. å®Œæ•´å¯è¿½æº¯ï¼šæ¯ä¸ª batch çš„æ•°æ®å’Œ loss éƒ½å•ç‹¬ä¿å­˜
    4. çŠ¶æ€å¯æ¢å¤ï¼šæ”¯æŒå®éªŒä¸­æ–­åç»§ç»­
    5. åªè®­ç»ƒ Assistant è¾“å‡ºï¼šUser input éƒ¨åˆ†ä¸å‚ä¸ loss è®¡ç®—
    """
    def __init__(
        self,
        batch_size: int = 8,
        sft_data_dir: str = "outputs/{TIMESTAMP}/sft_data",
        loss_log_path: str = "outputs/{TIMESTAMP}/loss_log.csv",
        lora_r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.1,
        lora_target_modules: Optional[List[str]] = None,
        learning_rate: float = 2e-4,
        num_train_epochs: int = 1,
        per_device_train_batch_size: int = 1,
        gradient_accumulation_steps: int = 4,
        max_seq_length: int = 2048,
    ):
        super().__init__()
        self.batch_size = batch_size
        # ä¿å­˜åŸå§‹è·¯å¾„æ¨¡æ¿ï¼ˆå¸¦å ä½ç¬¦ï¼‰
        self.sft_data_dir_template = sft_data_dir
        self.loss_log_path_template = loss_log_path
        # å®é™…è·¯å¾„å°†åœ¨é¦–æ¬¡è¿è¡Œæ—¶åˆå§‹åŒ–
        self.sft_data_dir = None
        self.loss_log_path = None
        self.trainer_output_dir = None  # æ–°å¢ï¼šTrainerä¸´æ—¶ç›®å½•
        self.max_seq_length = max_seq_length
        
        # LoRA é…ç½®
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        # å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨é€šç”¨çš„ target_modulesï¼ˆé€‚é…å¤§å¤šæ•°æ¨¡å‹ï¼‰
        self.lora_target_modules = lora_target_modules or [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"  # é€‚é… Llama/Qwen çš„ MLP
        ]
        
        # è®­ç»ƒé…ç½®
        self.learning_rate = learning_rate
        self.num_train_epochs = num_train_epochs
        self.per_device_train_batch_size = per_device_train_batch_size
        self.gradient_accumulation_steps = gradient_accumulation_steps
        
        # å†…éƒ¨çŠ¶æ€
        self.successful_trajectories: List[Session] = []
        self.model_ref = None
        self.tokenizer_ref = None
        self.is_lora_applied = False
        self.training_batch_count = 0
        self.paths_initialized = False  # âœ… æ·»åŠ è¿™ä¸€è¡Œï¼

    @classmethod
    def is_unique(cls) -> bool:
        return True

    def on_task_complete(self, callback_args: CallbackArguments) -> None:
        """åœ¨æ¯ä¸ªæ ·æœ¬å®Œæˆåï¼Œæ£€æŸ¥æ˜¯å¦æˆåŠŸå¹¶æ”¶é›†è½¨è¿¹"""
        session = callback_args.current_session
        # ä»…åœ¨å‰ 100 æ¡æ ·æœ¬ä¸Šè¿›è¡Œ SFT
        try:
            if int(session.sample_index) >= 100:
                return
        except Exception:
            pass
        
        # åªæ”¶é›†æˆåŠŸçš„è½¨è¿¹
        if (
            session.evaluation_record.outcome == SessionEvaluationOutcome.CORRECT and
            session.sample_status == SampleStatus.COMPLETED
        ):
            print(f"âœ… [TTT-AssistantOnly] æˆåŠŸè½¨è¿¹æ”¶é›†: sample_index={session.sample_index}")
            self.successful_trajectories.append(session.model_copy(deep=True))
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ° batch_size
            if len(self.successful_trajectories) >= self.batch_size:
                print(f"\n{'='*60}")
                print(f"ğŸ¯ [TTT-AssistantOnly] å·²æ”¶é›† {len(self.successful_trajectories)} æ¡æˆåŠŸè½¨è¿¹")
                print(f"{'='*60}\n")
                self._run_training(callback_args)

    def _run_training(self, callback_args: CallbackArguments):
        """æ‰§è¡Œä¸€æ¬¡ LoRA å¾®è°ƒ"""
        # é¦–å…ˆåˆå§‹åŒ–è·¯å¾„
        self._initialize_paths()
        
        print(f"ğŸš€ [TTT-AssistantOnly] å¼€å§‹ç¬¬ {self.training_batch_count + 1} è½® LoRA è®­ç»ƒ...")
        
        # 1. è·å–æ¨¡å‹å’Œ tokenizer å¼•ç”¨
        if not self._initialize_model_refs(callback_args):
            return
        
        # 2. é¦–æ¬¡åº”ç”¨ LoRAï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è®­ç»ƒæ—¶ï¼‰
        if not self.is_lora_applied:
            self._apply_lora_to_model()
        
        # 3. ä¿å­˜å½“å‰ batch çš„è®­ç»ƒæ•°æ®
        self._save_sft_batch_data()
        
        # 4. å‡†å¤‡æ•°æ®é›†
        train_dataset = SftTrajectoryDataset(
            self.successful_trajectories,
            self.tokenizer_ref,
            max_length=self.max_seq_length
        )
        
        # 5. é…ç½®è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.trainer_output_dir,  # âœ… æ‰€æœ‰batchå…±ç”¨ä¸€ä¸ªç›®å½•
            num_train_epochs=self.num_train_epochs,
            per_device_train_batch_size=self.per_device_train_batch_size,
            gradient_accumulation_steps=self.gradient_accumulation_steps,
            learning_rate=self.learning_rate,
            logging_steps=1,
            save_strategy="no",
            report_to="none",
            remove_unused_columns=False,
            overwrite_output_dir=True,  # âœ… å…è®¸è¦†ç›–ï¼ˆå› ä¸ºå…±ç”¨ç›®å½•ï¼‰
        )
        
        # 6. åˆ›å»º Trainer å¹¶æ³¨å…¥ loss æ—¥å¿—å›è°ƒ
        loss_logger = LossLoggingCallback(self.loss_log_path, self.training_batch_count)
        trainer = Trainer(
            model=self.model_ref,
            args=training_args,
            train_dataset=train_dataset,
            callbacks=[loss_logger]
        )
        
        # 7. å¼€å§‹è®­ç»ƒ
        print(f"ğŸ‹ï¸  [TTT-AssistantOnly] è®­ç»ƒä¸­...")
        train_result = trainer.train()
        print(f"âœ… [TTT-AssistantOnly] è®­ç»ƒå®Œæˆ! Loss: {train_result.training_loss:.4f}")
        print(f"ğŸ“Š [TTT-AssistantOnly] Loss å·²è®°å½•è‡³: {self.loss_log_path}")
        
        # 8. æ¸…ç†å½“å‰ batchï¼Œå‡†å¤‡ä¸‹ä¸€è½®
        self.successful_trajectories = []
        self.training_batch_count += 1
        print(f"ğŸ”„ [TTT-AssistantOnly] æ¸…ç†å®Œæˆï¼Œç»§ç»­æ”¶é›†ä¸‹ä¸€æ‰¹è½¨è¿¹\n")
        
    def _initialize_paths(self):
        """åˆå§‹åŒ–å®é™…çš„è¾“å‡ºè·¯å¾„ï¼ˆä»state_diræå–ï¼‰"""
        if self.paths_initialized:
            return
        
        # ä» state_dir æå–å®é™…çš„ output_dir
        # state_dir æ ¼å¼: outputs/2025-10-14-13-24-48/callback_state/callback_3
        state_dir = self.get_state_dir()
        output_dir = os.path.dirname(os.path.dirname(state_dir))  # å‘ä¸Šä¸¤çº§
        
        # æ›¿æ¢å ä½ç¬¦ï¼Œä½¿ç”¨å®é™…çš„æ—¶é—´æˆ³ç›®å½•
        self.sft_data_dir = os.path.join(output_dir, "sft_data")
        self.loss_log_path = os.path.join(output_dir, "loss_log.csv")
        # æ‰€æœ‰batchå…±ç”¨ä¸€ä¸ªä¸´æ—¶ç›®å½•ï¼ˆé¿å…äº§ç”Ÿå¤§é‡ç©ºç›®å½•ï¼‰
        self.trainer_output_dir = os.path.join(output_dir, ".ttt_trainer_temp")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(self.sft_data_dir, exist_ok=True)
        os.makedirs(os.path.dirname(self.loss_log_path), exist_ok=True)
        os.makedirs(self.trainer_output_dir, exist_ok=True)
        
        print(f"ğŸ“ [TTT-AssistantOnly] è¾“å‡ºç›®å½•å·²åˆå§‹åŒ–:")
        print(f"   - SFTæ•°æ®: {self.sft_data_dir}")
        print(f"   - Lossæ—¥å¿—: {self.loss_log_path}")
        print(f"   - Trainerä¸´æ—¶ç›®å½•: {self.trainer_output_dir}")
        
        self.paths_initialized = True

    def _initialize_model_refs(self, callback_args: CallbackArguments) -> bool:
        """åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer çš„å¼•ç”¨"""
        if self.model_ref is not None:
            return True
        
        agent = callback_args.session_context.agent
        if not hasattr(agent, "_language_model"):
            print("âš ï¸  [TTT-AssistantOnly] Agent æ²¡æœ‰ _language_model å±æ€§ï¼Œè·³è¿‡è®­ç»ƒ")
            return False
        
        if not isinstance(agent._language_model, HuggingfaceLanguageModel):
            print("âš ï¸  [TTT-AssistantOnly] åªæ”¯æŒ HuggingfaceLanguageModelï¼Œè·³è¿‡è®­ç»ƒ")
            return False
        
        self.model_ref = agent._language_model.model
        self.tokenizer_ref = agent._language_model.tokenizer
        
        # æ£€æµ‹æ¨¡å‹ç±»å‹ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        model_type = self.model_ref.config.model_type
        print(f"ğŸ” [TTT-AssistantOnly] æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type}")
        
        return True

    def _apply_lora_to_model(self):
        """é¦–æ¬¡åº”ç”¨ LoRA é€‚é…å™¨åˆ°æ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"ğŸ”§ [TTT-AssistantOnly] é¦–æ¬¡åº”ç”¨ LoRA é€‚é…å™¨")
        print(f"{'='*60}")
        
        # åˆ›å»º LoRA é…ç½®
        lora_config = LoraConfig(
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            target_modules=self.lora_target_modules,
            lora_dropout=self.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # åº”ç”¨ LoRA
        self.model_ref = get_peft_model(self.model_ref, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
        self.model_ref.print_trainable_parameters()
        
        # å…³é”®ï¼šæ›´æ–° agent ä¸­çš„æ¨¡å‹å¼•ç”¨
        # è¿™æ ·åç»­çš„æ¨ç†ä¼šä½¿ç”¨å¸¦ LoRA çš„æ¨¡å‹
        agent = self.model_ref  # å·²ç»æ˜¯ PeftModel äº†
        
        self.is_lora_applied = True
        print(f"âœ… [TTT-AssistantOnly] LoRA é€‚é…å™¨å·²æˆåŠŸåº”ç”¨\n")

    def _save_sft_batch_data(self):
        """ä¿å­˜å½“å‰ batch çš„ SFT æ•°æ®"""
        batch_file = os.path.join(
            self.sft_data_dir,
            f"batch_{self.training_batch_count:03d}.json"
        )
        os.makedirs(os.path.dirname(batch_file), exist_ok=True)
        
        # ä¿å­˜å®Œæ•´çš„ Session æ•°æ®
        batch_data = {
            "batch_id": self.training_batch_count,
            "sample_count": len(self.successful_trajectories),
            "sample_indices": [s.sample_index for s in self.successful_trajectories],
            "trajectories": [s.model_dump() for s in self.successful_trajectories]
        }
        
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(batch_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ [TTT-AssistantOnly] Batch {self.training_batch_count} æ•°æ®å·²ä¿å­˜: {batch_file}")

    def on_state_save(self, callback_args: CallbackArguments) -> None:
        """ä¿å­˜å›è°ƒçŠ¶æ€ï¼ˆæ”¯æŒæ–­ç‚¹æ¢å¤ï¼‰"""
        state = {
            "training_batch_count": self.training_batch_count,
            "is_lora_applied": self.is_lora_applied,
            "successful_trajectories": [s.model_dump() for s in self.successful_trajectories]
        }
        state_path = os.path.join(self.get_state_dir(), "ttt_state.json")
        with open(state_path, 'w') as f:
            json.dump(state, f, indent=2)

    def restore_state(self) -> None:
        """æ¢å¤å›è°ƒçŠ¶æ€"""
        # å…ˆåˆå§‹åŒ–è·¯å¾„
        self._initialize_paths()
        
        state_path = os.path.join(self.get_state_dir(), "ttt_state.json")
        if os.path.exists(state_path):
            with open(state_path, 'r') as f:
                state = json.load(f)
            self.training_batch_count = state.get("training_batch_count", 0)
            self.is_lora_applied = state.get("is_lora_applied", False)
            self.successful_trajectories = [
                Session.model_validate(s)
                for s in state.get("successful_trajectories", [])
            ]
            print(f"ğŸ”„ [TTT-AssistantOnly] çŠ¶æ€å·²æ¢å¤: å·²å®Œæˆ {self.training_batch_count} è½®è®­ç»ƒï¼Œ"
                f"å½“å‰æ”¶é›† {len(self.successful_trajectories)} æ¡è½¨è¿¹")

